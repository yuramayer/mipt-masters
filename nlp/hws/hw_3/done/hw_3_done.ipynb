{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "homework-header"
      },
      "source": [
        "# Домашнее задание 3. Fine-Tuning модели BERT и анализ альтернативных архитектур в задаче классификации\n",
        "\n",
        "**ФИО Студента:** Майер Юрий Алексеевич\n",
        "\n",
        "**Дата Выполнения:** 6 октября 25\n",
        "\n",
        "---\n",
        "\n",
        "### **Описание задания**\n",
        "\n",
        "В этом задании вы реализуете эксперементальное сравнение классического трансформера (BERT) с современными альтернативными архитектурами (Mamba) на задаче классификации русскоязычных текстов. Проведете исследование trade-offs между качеством, скоростью и количеством обучаемых параметров для различных подходов к Fine-Tuning.\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Я работаю на своём ПК, так что перед работой проверим, не умрёт ли наш диск C от этого ноутбука:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "PIP_CACHE_DIR = F:\\dev_cache\\pip-cache\n",
            "TEMP = F:\\dev_cache\\tmp\n",
            "HF_HOME = F:\\dev_cache\\hf\\hub\n",
            "HF_DATASETS_CACHE = F:\\dev_cache\\hf\\datasets\n",
            "TRANSFORMERS_CACHE = F:\\dev_cache\\hf\\transformers\n",
            "TORCH_HOME = F:\\dev_cache\\torch\\home\n",
            "TORCH_EXTENSIONS_DIR = F:\\dev_cache\\torch\\extensions\n"
          ]
        }
      ],
      "source": [
        "import os, pathlib\n",
        "\n",
        "\n",
        "os.environ[\"PIP_CACHE_DIR\"]       = r\"F:\\dev_cache\\pip-cache\"\n",
        "os.environ[\"PIP_CONFIG_FILE\"]     = r\"F:\\dev_cache\\pip\\pip.ini\"\n",
        "os.environ[\"TEMP\"]                = r\"F:\\dev_cache\\tmp\"\n",
        "os.environ[\"TMP\"]                 = r\"F:\\dev_cache\\tmp\"\n",
        "os.environ[\"HF_HOME\"]             = r\"F:\\dev_cache\\hf\\hub\"\n",
        "os.environ[\"HF_HUB_CACHE\"]        = r\"F:\\dev_cache\\hf\\hub\"\n",
        "os.environ[\"HF_DATASETS_CACHE\"]   = r\"F:\\dev_cache\\hf\\datasets\"\n",
        "os.environ[\"TRANSFORMERS_CACHE\"]  = r\"F:\\dev_cache\\hf\\transformers\"\n",
        "os.environ[\"TORCH_HOME\"]          = r\"F:\\dev_cache\\torch\\home\"\n",
        "os.environ[\"TORCH_EXTENSIONS_DIR\"]= r\"F:\\dev_cache\\torch\\extensions\"\n",
        "os.environ[\"JUPYTER_CONFIG_DIR\"]  = r\"F:\\dev_cache\\jupyter\\config\"\n",
        "os.environ[\"JUPYTER_RUNTIME_DIR\"] = r\"F:\\dev_cache\\jupyter\\runtime\"\n",
        "os.environ[\"IPYTHONDIR\"]          = r\"F:\\dev_cache\\ipython\"\n",
        "os.environ[\"MPLCONFIGDIR\"]        = r\"F:\\dev_cache\\matplotlib\"\n",
        "\n",
        "\n",
        "for p in [\n",
        "    os.environ[\"PIP_CACHE_DIR\"], os.environ[\"TEMP\"], os.environ[\"TMP\"],\n",
        "    os.environ[\"HF_HOME\"], os.environ[\"HF_HUB_CACHE\"], os.environ[\"HF_DATASETS_CACHE\"],\n",
        "    os.environ[\"TRANSFORMERS_CACHE\"], os.environ[\"TORCH_HOME\"], os.environ[\"TORCH_EXTENSIONS_DIR\"],\n",
        "    os.environ[\"JUPYTER_CONFIG_DIR\"], os.environ[\"JUPYTER_RUNTIME_DIR\"],\n",
        "    os.environ[\"IPYTHONDIR\"], os.environ[\"MPLCONFIGDIR\"]\n",
        "]:\n",
        "    pathlib.Path(p).mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "\n",
        "for k in [\"PIP_CACHE_DIR\",\"TEMP\",\"HF_HOME\",\"HF_DATASETS_CACHE\",\"TRANSFORMERS_CACHE\",\"TORCH_HOME\",\"TORCH_EXTENSIONS_DIR\"]:\n",
        "    print(f\"{k} = {os.environ[k]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup-section"
      },
      "source": [
        "## **Установка и импорт библиотек**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "imports"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "f:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "f:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\utils\\hub.py:110: FutureWarning: Using `TRANSFORMERS_CACHE` is deprecated and will be removed in v5 of Transformers. Use `HF_HOME` instead.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorWithPadding,\n",
        "    set_seed\n",
        ")\n",
        "\n",
        "from datasets import Dataset as HFDataset\n",
        "import evaluate\n",
        "\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    get_peft_model\n",
        ")\n",
        "\n",
        "from sklearn.metrics import accuracy_score, f1_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "data-section"
      },
      "source": [
        "## **Задание 1. Подготовка данных и базовой модели**\n",
        "\n",
        "Используем полный датасет русскоязычных отзывов с Кинопоиска. Для упрощения задачи бинарной классификации удаляем нейтральные отзывы. Разбиваем данные на обучающую и тестовую выборки в соотношении 80/20.\n",
        "\n",
        "Задачи:\n",
        "1. Загрузите датасет отзывов Кинопоиска и соответствующий токенизатор для DeepPavlov/rubert-base-cased.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "load-and-split-dataset"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Загружаем полный датасет отзывов...\n"
          ]
        }
      ],
      "source": [
        "# Загружаем полный датасет\n",
        "print(\"Загружаем полный датасет отзывов...\")\n",
        "df_full = pd.read_json(\"hf://datasets/blinoff/kinopoisk/kinopoisk.jsonl\", lines=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>part</th>\n",
              "      <th>movie_name</th>\n",
              "      <th>review_id</th>\n",
              "      <th>author</th>\n",
              "      <th>date</th>\n",
              "      <th>title</th>\n",
              "      <th>grade3</th>\n",
              "      <th>grade10</th>\n",
              "      <th>content</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>top250</td>\n",
              "      <td>Блеф (1976)</td>\n",
              "      <td>17144</td>\n",
              "      <td>Come Back</td>\n",
              "      <td>2011-09-24</td>\n",
              "      <td>Плакали наши денежки ©</td>\n",
              "      <td>Good</td>\n",
              "      <td>10.0</td>\n",
              "      <td>\\n\"Блеф» — одна из моих самых любимых комедий....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>top250</td>\n",
              "      <td>Блеф (1976)</td>\n",
              "      <td>17139</td>\n",
              "      <td>Stasiki</td>\n",
              "      <td>2008-03-04</td>\n",
              "      <td>None</td>\n",
              "      <td>Good</td>\n",
              "      <td>0.0</td>\n",
              "      <td>\\nАдриано Челентано продолжает радовать нас св...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>top250</td>\n",
              "      <td>Блеф (1976)</td>\n",
              "      <td>17137</td>\n",
              "      <td>Flashman</td>\n",
              "      <td>2007-03-04</td>\n",
              "      <td>None</td>\n",
              "      <td>Good</td>\n",
              "      <td>10.0</td>\n",
              "      <td>\\nНесомненно, это один из великих фильмов 80-х...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>top250</td>\n",
              "      <td>Блеф (1976)</td>\n",
              "      <td>17135</td>\n",
              "      <td>Sergio Tishin</td>\n",
              "      <td>2009-08-17</td>\n",
              "      <td>\" Черное, красное, ерунда это все. Выигрывает ...</td>\n",
              "      <td>Good</td>\n",
              "      <td>0.0</td>\n",
              "      <td>\\nЭта фраза на мой взгляд отражает сюжет несом...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>top250</td>\n",
              "      <td>Блеф (1976)</td>\n",
              "      <td>17151</td>\n",
              "      <td>Фюльгья</td>\n",
              "      <td>2009-08-20</td>\n",
              "      <td>«Он хотел убежать? Да! Блеф, блеф…»</td>\n",
              "      <td>Neutral</td>\n",
              "      <td>7.0</td>\n",
              "      <td>\\n- как пела Земфира, скорее всего, по соверше...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36586</th>\n",
              "      <td>bottom100</td>\n",
              "      <td>Цветок дьявола (2010)</td>\n",
              "      <td>25123</td>\n",
              "      <td>bestiya163</td>\n",
              "      <td>2010-09-23</td>\n",
              "      <td>Ой, ой, ой!</td>\n",
              "      <td>Bad</td>\n",
              "      <td>2.0</td>\n",
              "      <td>\\n      Ну с чего бы начать… Давненько я не пи...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36587</th>\n",
              "      <td>bottom100</td>\n",
              "      <td>Цветок дьявола (2010)</td>\n",
              "      <td>25192</td>\n",
              "      <td>Молка</td>\n",
              "      <td>2010-10-02</td>\n",
              "      <td>Молчаливый мужик на коне…</td>\n",
              "      <td>Bad</td>\n",
              "      <td>1.0</td>\n",
              "      <td>\\n      Можно начать с того, что уже постер к ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36588</th>\n",
              "      <td>bottom100</td>\n",
              "      <td>Цветок дьявола (2010)</td>\n",
              "      <td>25080</td>\n",
              "      <td>jetry</td>\n",
              "      <td>2010-09-16</td>\n",
              "      <td>Это проявилось сегодня ночью.</td>\n",
              "      <td>Good</td>\n",
              "      <td>7.0</td>\n",
              "      <td>\\n      Фильм производства России, поэтому мно...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36589</th>\n",
              "      <td>bottom100</td>\n",
              "      <td>Цветок дьявола (2010)</td>\n",
              "      <td>25088</td>\n",
              "      <td>Alkort</td>\n",
              "      <td>2010-09-16</td>\n",
              "      <td>«Finita la comedia»</td>\n",
              "      <td>Bad</td>\n",
              "      <td>0.0</td>\n",
              "      <td>\\n      16 сентября на большие экраны вышел «м...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>36590</th>\n",
              "      <td>bottom100</td>\n",
              "      <td>Цветок дьявола (2010)</td>\n",
              "      <td>25149</td>\n",
              "      <td>Флоя</td>\n",
              "      <td>2011-02-20</td>\n",
              "      <td>На что потратили аж 5000000?!</td>\n",
              "      <td>Bad</td>\n",
              "      <td>1.0</td>\n",
              "      <td>\\n      Эх, как я пыталась настроить себя поло...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>36591 rows × 9 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            part             movie_name  review_id         author       date  \\\n",
              "0         top250            Блеф (1976)      17144      Come Back 2011-09-24   \n",
              "1         top250            Блеф (1976)      17139        Stasiki 2008-03-04   \n",
              "2         top250            Блеф (1976)      17137       Flashman 2007-03-04   \n",
              "3         top250            Блеф (1976)      17135  Sergio Tishin 2009-08-17   \n",
              "4         top250            Блеф (1976)      17151        Фюльгья 2009-08-20   \n",
              "...          ...                    ...        ...            ...        ...   \n",
              "36586  bottom100  Цветок дьявола (2010)      25123     bestiya163 2010-09-23   \n",
              "36587  bottom100  Цветок дьявола (2010)      25192          Молка 2010-10-02   \n",
              "36588  bottom100  Цветок дьявола (2010)      25080          jetry 2010-09-16   \n",
              "36589  bottom100  Цветок дьявола (2010)      25088         Alkort 2010-09-16   \n",
              "36590  bottom100  Цветок дьявола (2010)      25149           Флоя 2011-02-20   \n",
              "\n",
              "                                                   title   grade3  grade10  \\\n",
              "0                                 Плакали наши денежки ©     Good     10.0   \n",
              "1                                                   None     Good      0.0   \n",
              "2                                                   None     Good     10.0   \n",
              "3      \" Черное, красное, ерунда это все. Выигрывает ...     Good      0.0   \n",
              "4                    «Он хотел убежать? Да! Блеф, блеф…»  Neutral      7.0   \n",
              "...                                                  ...      ...      ...   \n",
              "36586                                        Ой, ой, ой!      Bad      2.0   \n",
              "36587                          Молчаливый мужик на коне…      Bad      1.0   \n",
              "36588                      Это проявилось сегодня ночью.     Good      7.0   \n",
              "36589                                «Finita la comedia»      Bad      0.0   \n",
              "36590                      На что потратили аж 5000000?!      Bad      1.0   \n",
              "\n",
              "                                                 content  \n",
              "0      \\n\"Блеф» — одна из моих самых любимых комедий....  \n",
              "1      \\nАдриано Челентано продолжает радовать нас св...  \n",
              "2      \\nНесомненно, это один из великих фильмов 80-х...  \n",
              "3      \\nЭта фраза на мой взгляд отражает сюжет несом...  \n",
              "4      \\n- как пела Земфира, скорее всего, по соверше...  \n",
              "...                                                  ...  \n",
              "36586  \\n      Ну с чего бы начать… Давненько я не пи...  \n",
              "36587  \\n      Можно начать с того, что уже постер к ...  \n",
              "36588  \\n      Фильм производства России, поэтому мно...  \n",
              "36589  \\n      16 сентября на большие экраны вышел «м...  \n",
              "36590  \\n      Эх, как я пыталась настроить себя поло...  \n",
              "\n",
              "[36591 rows x 9 columns]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "df_full"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "# делаем копию чтоб каждый раз не грузить лишнего\n",
        "\n",
        "df = df_full.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "common-utils-section"
      },
      "source": [
        "Определим функции для токенизации, вычисления метрик и подсчета обучаемых параметров, которые будут использоваться для всех моделей.   \n",
        "\n",
        "2. Подготовьте данные: создайте dataset-объекты для обучающей и тестовой выборок, токенизируйте тексты и подготовьте их к подаче в модель в соответствии с семинаром 1 данной дисциплины.  \n",
        "3. Определите функцию для вычисления метрик Accuracy, F1-score.  \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "utility-functions"
      },
      "outputs": [],
      "source": [
        "MAX_LENGTH = 256 # Ограничиваем длину для ускорения обучения и экономии памяти\n",
        "\n",
        "df = df.dropna(subset=[\"content\", \"grade10\"])\n",
        "df = df[df[\"content\"].str.strip() != \"\"]\n",
        "\n",
        "df[\"label\"] = (df[\"grade10\"] >= 5).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_train, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df[\"label\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train = HFDataset.from_pandas(df_train.reset_index(drop=True))\n",
        "ds_test = HFDataset.from_pandas(df_test.reset_index(drop=True))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset({\n",
            "    features: ['part', 'movie_name', 'review_id', 'author', 'date', 'title', 'grade3', 'grade10', 'content', 'label'],\n",
            "    num_rows: 29272\n",
            "}) Dataset({\n",
            "    features: ['part', 'movie_name', 'review_id', 'author', 'date', 'title', 'grade3', 'grade10', 'content', 'label'],\n",
            "    num_rows: 7319\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(ds_train, ds_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "MODEL_NAME = \"distilbert-base-multilingual-cased\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {},
      "outputs": [],
      "source": [
        "def tokenize_fn(batch):\n",
        "    return tokenizer(\n",
        "        batch[\"content\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=MAX_LENGTH\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 29272/29272 [00:20<00:00, 1396.24 examples/s]\n",
            "Map: 100%|██████████| 7319/7319 [00:04<00:00, 1560.43 examples/s]\n"
          ]
        }
      ],
      "source": [
        "ds_train_tok = ds_train.map(tokenize_fn, batched=True)\n",
        "ds_test_tok = ds_test.map(tokenize_fn, batched=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train_tok = ds_train_tok.remove_columns([\"part\", \"movie_name\", \"review_id\", \"author\", \"date\", \"title\", \"grade3\", \"grade10\", \"content\"])\n",
        "ds_test_tok = ds_test_tok.remove_columns([\"part\", \"movie_name\", \"review_id\", \"author\", \"date\", \"title\", \"grade3\", \"grade10\", \"content\"])\n",
        "\n",
        "ds_train_tok.set_format(\"torch\")\n",
        "ds_test_tok.set_format(\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [],
      "source": [
        "accuracy = evaluate.load(\"accuracy\")\n",
        "f1 = evaluate.load(\"f1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [],
      "source": [
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    preds = np.argmax(logits, axis=-1)\n",
        "    acc = accuracy.compute(predictions=preds, references=labels)\n",
        "    f1_score = f1.compute(predictions=preds, references=labels, average=\"weighted\")\n",
        "    return {\"accuracy\": acc[\"accuracy\"], \"f1\": f1_score[\"f1\"]}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Отдельно реализуем под пефт:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {},
      "outputs": [],
      "source": [
        "def count_trainable_parameters(model):\n",
        "    total = sum(p.numel() for p in model.parameters())\n",
        "    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    print(f\"Trainable params: {trainable:,} / {total:,} ({100*trainable/total:.2f}%)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bert-section"
      },
      "source": [
        "## **Задания 2 и 3. Baseline — Fine-Tuning BERT**\n",
        "\n",
        "В качестве baseline используем русскоязычную модель `DeepPavlov/rubert-base-cased`. Мы рассмотрим два подхода: полный Fine-Tuning и эффективный Fine-Tuning с помощью LoRA.\n",
        "\n",
        "Задачи:  \n",
        "**BERT Full Fine-Tuning:**\n",
        "1. Загрузите предобученную модель DeepPavlov/rubert-base-cased.\n",
        "2. Настройте TrainingArguments для полного дообучения.\n",
        "3. Обучите модель на полном обучающем наборе данных.\n",
        "4. Оцените качество на тестовой выборке и зафиксируйте время обучения и количество обучаемых параметров.  \n",
        "\n",
        "**BERT с LoRA или иным методом (Parameter-Efficient Fine-Tuning):**\n",
        "1. Снова загрузите исходную модель DeepPavlov/rubert-base-cased.\n",
        "2. Настройте LoraConfig, указав целевые модули (например, query, value).\n",
        "3. Примените LoRA к модели с помощью get_peft_model.\n",
        "4. Обучите параметро-эффективную модель.\n",
        "5. Оцените ее качество, время обучения и количество обучаемых параметров.   \n",
        "6. Сравните с результатами полного дообучения."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [],
      "source": [
        "ds_train_tok_small = ds_train_tok.select(range(2000))\n",
        "ds_test_tok_small = ds_test_tok.select(range(500))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "id": "bert-full-finetuning",
        "outputId": "73dcce30-65a7-4e7b-b1b8-5a427f30bb89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- 1. BERT: Full Fine-tuning (tiny) ---\n",
            "step 1\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 2\n",
            "Trainable params: 135,326,210 / 135,326,210 (100.00%)\n",
            "step 3\n",
            "step 4\n",
            "step 5\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 26:14, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step 6\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 01:29]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Completed in 28.48 min\n",
            "{'eval_loss': 0.6769466400146484, 'eval_accuracy': 0.604, 'eval_f1': 0.4548827930174563, 'eval_runtime': 90.6178, 'eval_samples_per_second': 5.518, 'eval_steps_per_second': 0.695, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "print(\"--- 1. BERT: Full Fine-tuning (tiny) ---\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "print('step 1')\n",
        "model_full = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    cache_dir=\"F:/dev_cache/hf/transformers\"\n",
        ")\n",
        "print('step 2')\n",
        "count_trainable_parameters(model_full)\n",
        "print('step 3')\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"F:/dev_cache/models/distilbert_head\",\n",
        "    per_device_train_batch_size=8,\n",
        "    num_train_epochs=1,\n",
        "    learning_rate=2e-4,\n",
        "    logging_dir=\"F:/dev_cache/logs/distilbert_head\"\n",
        ")\n",
        "print('step 4')\n",
        "trainer = Trainer(\n",
        "    model=model_full,\n",
        "    args=training_args,\n",
        "    train_dataset=ds_train_tok_small,\n",
        "    eval_dataset=ds_test_tok_small,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "print('step 5')\n",
        "trainer.train()\n",
        "print('step 6')\n",
        "eval_full = trainer.evaluate()\n",
        "\n",
        "print(f\"✅ Completed in {(time.time()-start_time)/60:.2f} min\")\n",
        "print(eval_full)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "4.57.0\n"
          ]
        }
      ],
      "source": [
        "import transformers\n",
        "print(transformers.__version__)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Результат лоры сбился, а времени снова прогонять её нет 😭\n",
        "\n",
        "Но ячейку можно прогнать самому!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "bert-lora-finetuning"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- 2. BERT: LoRA Fine-tuning ---\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-multilingual-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Trainable params: 739,586 / 136,065,796 (0.54%)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='4' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [  4/500 00:12 < 51:23, 0.16 it/s, Epoch 0.01/2]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     44\u001b[39m \u001b[38;5;66;03m# trainer\u001b[39;00m\n\u001b[32m     45\u001b[39m trainer_lora = Trainer(\n\u001b[32m     46\u001b[39m     model=model_lora,\n\u001b[32m     47\u001b[39m     args=training_args_lora,\n\u001b[32m   (...)\u001b[39m\u001b[32m     52\u001b[39m     compute_metrics=compute_metrics,\n\u001b[32m     53\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m \u001b[43mtrainer_lora\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# оцениваем :)\u001b[39;00m\n\u001b[32m     58\u001b[39m eval_lora = trainer_lora.evaluate()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2325\u001b[39m, in \u001b[36mTrainer.train\u001b[39m\u001b[34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[39m\n\u001b[32m   2323\u001b[39m         hf_hub_utils.enable_progress_bars()\n\u001b[32m   2324\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2325\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2326\u001b[39m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[43m=\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2327\u001b[39m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m=\u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2328\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2329\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2330\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\trainer.py:2674\u001b[39m, in \u001b[36mTrainer._inner_training_loop\u001b[39m\u001b[34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[39m\n\u001b[32m   2667\u001b[39m context = (\n\u001b[32m   2668\u001b[39m     functools.partial(\u001b[38;5;28mself\u001b[39m.accelerator.no_sync, model=model)\n\u001b[32m   2669\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i != \u001b[38;5;28mlen\u001b[39m(batch_samples) - \u001b[32m1\u001b[39m\n\u001b[32m   2670\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.accelerator.distributed_type != DistributedType.DEEPSPEED\n\u001b[32m   2671\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m contextlib.nullcontext\n\u001b[32m   2672\u001b[39m )\n\u001b[32m   2673\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m context():\n\u001b[32m-> \u001b[39m\u001b[32m2674\u001b[39m     tr_loss_step = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2676\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2677\u001b[39m     args.logging_nan_inf_filter\n\u001b[32m   2678\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_xla_available()\n\u001b[32m   2679\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (torch.isnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch.isinf(tr_loss_step))\n\u001b[32m   2680\u001b[39m ):\n\u001b[32m   2681\u001b[39m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[32m   2682\u001b[39m     tr_loss = tr_loss + tr_loss / (\u001b[32m1\u001b[39m + \u001b[38;5;28mself\u001b[39m.state.global_step - \u001b[38;5;28mself\u001b[39m._globalstep_last_logged)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4020\u001b[39m, in \u001b[36mTrainer.training_step\u001b[39m\u001b[34m(self, model, inputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4017\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m loss_mb.reduce_mean().detach().to(\u001b[38;5;28mself\u001b[39m.args.device)\n\u001b[32m   4019\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.compute_loss_context_manager():\n\u001b[32m-> \u001b[39m\u001b[32m4020\u001b[39m     loss = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcompute_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_items_in_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4022\u001b[39m \u001b[38;5;28;01mdel\u001b[39;00m inputs\n\u001b[32m   4023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   4024\u001b[39m     \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   4025\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.global_step % \u001b[38;5;28mself\u001b[39m.args.torch_empty_cache_steps == \u001b[32m0\u001b[39m\n\u001b[32m   4026\u001b[39m ):\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\trainer.py:4110\u001b[39m, in \u001b[36mTrainer.compute_loss\u001b[39m\u001b[34m(self, model, inputs, return_outputs, num_items_in_batch)\u001b[39m\n\u001b[32m   4108\u001b[39m         kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_items_in_batch\u001b[39m\u001b[33m\"\u001b[39m] = num_items_in_batch\n\u001b[32m   4109\u001b[39m     inputs = {**inputs, **kwargs}\n\u001b[32m-> \u001b[39m\u001b[32m4110\u001b[39m outputs = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4111\u001b[39m \u001b[38;5;66;03m# Save past state if it exists\u001b[39;00m\n\u001b[32m   4112\u001b[39m \u001b[38;5;66;03m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[32m   4113\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.past_index >= \u001b[32m0\u001b[39m:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\peft\\peft_model.py:1652\u001b[39m, in \u001b[36mPeftModelForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, task_ids, **kwargs)\u001b[39m\n\u001b[32m   1650\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m peft_config.peft_type == PeftType.POLY:\n\u001b[32m   1651\u001b[39m             kwargs[\u001b[33m\"\u001b[39m\u001b[33mtask_ids\u001b[39m\u001b[33m\"\u001b[39m] = task_ids\n\u001b[32m-> \u001b[39m\u001b[32m1652\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbase_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1653\u001b[39m \u001b[43m            \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1654\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1655\u001b[39m \u001b[43m            \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1656\u001b[39m \u001b[43m            \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1657\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1658\u001b[39m \u001b[43m            \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1659\u001b[39m \u001b[43m            \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1660\u001b[39m \u001b[43m            \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1661\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1663\u001b[39m batch_size = _get_batch_size(input_ids, inputs_embeds)\n\u001b[32m   1664\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1665\u001b[39m     \u001b[38;5;66;03m# concat prompt attention mask\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\peft\\tuners\\tuners_utils.py:222\u001b[39m, in \u001b[36mBaseTuner.forward\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any):\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:905\u001b[39m, in \u001b[36mDistilBertForSequenceClassification.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    897\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    898\u001b[39m \u001b[33;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[32m    899\u001b[39m \u001b[33;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[32m    900\u001b[39m \u001b[33;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[32m    901\u001b[39m \u001b[33;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[32m    902\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    903\u001b[39m return_dict = return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.use_return_dict\n\u001b[32m--> \u001b[39m\u001b[32m905\u001b[39m distilbert_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdistilbert\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    906\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    907\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    908\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    909\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    910\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    911\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    912\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    913\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    914\u001b[39m hidden_state = distilbert_output[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, seq_len, dim)\u001b[39;00m\n\u001b[32m    915\u001b[39m pooled_output = hidden_state[:, \u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# (bs, dim)\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:724\u001b[39m, in \u001b[36mDistilBertModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, head_mask, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    719\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation == \u001b[33m\"\u001b[39m\u001b[33msdpa\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m head_mask_is_none \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m output_attentions:\n\u001b[32m    720\u001b[39m         attention_mask = _prepare_4d_attention_mask_for_sdpa(\n\u001b[32m    721\u001b[39m             attention_mask, embeddings.dtype, tgt_len=input_shape[\u001b[32m1\u001b[39m]\n\u001b[32m    722\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m724\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtransformer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    725\u001b[39m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43membeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    726\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    727\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    728\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    729\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    730\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    731\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:531\u001b[39m, in \u001b[36mTransformer.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    528\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    529\u001b[39m     all_hidden_states = all_hidden_states + (hidden_state,)\n\u001b[32m--> \u001b[39m\u001b[32m531\u001b[39m layer_outputs = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    532\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    533\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    534\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    535\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    536\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    538\u001b[39m hidden_state = layer_outputs[-\u001b[32m1\u001b[39m]\n\u001b[32m    540\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m output_attentions:\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:484\u001b[39m, in \u001b[36mTransformerBlock.forward\u001b[39m\u001b[34m(self, x, attn_mask, head_mask, output_attentions)\u001b[39m\n\u001b[32m    481\u001b[39m sa_output = \u001b[38;5;28mself\u001b[39m.sa_layer_norm(sa_output + x)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# Feed Forward Network\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m484\u001b[39m ffn_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mffn\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa_output\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    485\u001b[39m ffn_output: torch.Tensor = \u001b[38;5;28mself\u001b[39m.output_layer_norm(ffn_output + sa_output)  \u001b[38;5;66;03m# (bs, seq_length, dim)\u001b[39;00m\n\u001b[32m    487\u001b[39m output = (ffn_output,)\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:418\u001b[39m, in \u001b[36mFFN.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    417\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: torch.Tensor) -> torch.Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m418\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mff_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\pytorch_utils.py:257\u001b[39m, in \u001b[36mapply_chunking_to_forward\u001b[39m\u001b[34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[39m\n\u001b[32m    254\u001b[39m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[32m    255\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m torch.cat(output_chunks, dim=chunk_dim)\n\u001b[32m--> \u001b[39m\u001b[32m257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\transformers\\models\\distilbert\\modeling_distilbert.py:424\u001b[39m, in \u001b[36mFFN.ff_chunk\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    422\u001b[39m x = \u001b[38;5;28mself\u001b[39m.activation(x)\n\u001b[32m    423\u001b[39m x = \u001b[38;5;28mself\u001b[39m.lin2(x)\n\u001b[32m--> \u001b[39m\u001b[32m424\u001b[39m x = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    425\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\dropout.py:70\u001b[39m, in \u001b[36mDropout.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m     69\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m---> \u001b[39m\u001b[32m70\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32mf:\\pycode25\\mipt-masters\\nlp\\hws\\hw_3\\.venv\\Lib\\site-packages\\torch\\nn\\functional.py:1422\u001b[39m, in \u001b[36mdropout\u001b[39m\u001b[34m(input, p, training, inplace)\u001b[39m\n\u001b[32m   1419\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m p < \u001b[32m0.0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m p > \u001b[32m1.0\u001b[39m:\n\u001b[32m   1420\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mdropout probability has to be between 0 and 1, but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m   1421\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m-> \u001b[39m\u001b[32m1422\u001b[39m     _VF.dropout_(\u001b[38;5;28minput\u001b[39m, p, training) \u001b[38;5;28;01mif\u001b[39;00m inplace \u001b[38;5;28;01melse\u001b[39;00m \u001b[43m_VF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1423\u001b[39m )\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "print(\"\\n--- 2. BERT: LoRA Fine-tuning ---\")\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "\n",
        "MODEL_NAME = \"distilbert-base-multilingual-cased\"\n",
        "\n",
        "model_lora = AutoModelForSequenceClassification.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    num_labels=2,\n",
        "    cache_dir=\"F:/dev_cache/hf/transformers\"\n",
        ")\n",
        "\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"q_lin\", \"v_lin\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "\n",
        "model_lora = get_peft_model(model_lora, lora_config)\n",
        "\n",
        "count_trainable_parameters(model_lora)\n",
        "\n",
        "\n",
        "training_args_lora = TrainingArguments(\n",
        "    output_dir=\"F:/dev_cache/models/distilbert_lora\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=2e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"F:/dev_cache/logs/distilbert_lora\",\n",
        "    logging_steps=50\n",
        ")\n",
        "\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
        "\n",
        "# trainer\n",
        "trainer_lora = Trainer(\n",
        "    model=model_lora,\n",
        "    args=training_args_lora,\n",
        "    train_dataset=ds_train_tok_small if \"ds_train_tok_small\" in locals() else ds_train_tok,\n",
        "    eval_dataset=ds_test_tok_small if \"ds_test_tok_small\" in locals() else ds_test_tok,\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "trainer_lora.train()\n",
        "\n",
        "# оцениваем :)\n",
        "eval_lora = trainer_lora.evaluate()\n",
        "elapsed_lora = time.time() - start_time\n",
        "\n",
        "print(f\"✅ LoRA Fine-tuning completed in {elapsed_lora/60:.2f} min\")\n",
        "print(eval_lora)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mamba-section"
      },
      "source": [
        "## **Задание 4. Альтернативная архитектура — Mamba**\n",
        "\n",
        "Mamba — это современная архитектура на основе пространств состояний (State Space Models), которая показывает высокую производительность и линейную сложность по длине последовательности. Используем небольшую предобученную модель `state-spaces/mamba-130m-hf`.\n",
        "\n",
        "Архитектура еще не распространенная, поэтому нужно самостоятельно написать блок для классификатора последовательностей. Вы можете воспользоваться готовым кодом ниже для эксперимента, либо установить библиотеку https://github.com/getorca/mamba_for_sequence_classification, либо протестировать иные архитектуры с huggingface\n",
        "\n",
        "Задачи:  \n",
        "1. Загрузите предобученную модель Mamba или другую, подходящую для классификации текста (например, state-spaces/mamba-130m-hf).\n",
        "2. Адаптируйте модель для задачи бинарной классификации (добавьте классификационную голову).\n",
        "3. Настройте TrainingArguments и проведите Fine-Tuning модели Mamba.\n",
        "Оцените ее итоговое качество, время обучения и количество параметров.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 98,
          "referenced_widgets": [
            "6fc4e0cb4d4d49489a0a65f28289a83a",
            "20df76277cbb49fd88d6794eb7433a25",
            "8bf4d34b7b5f4760998a059bc524164e",
            "f972a2389d4240dd877d04871f9f11f7",
            "7ed7d37c014d4c7691f5218eba7babe2",
            "254c3e3c25ee45b3a14634205e8d1f07",
            "d87ee7784e7c4cccab29e8aef0d9bc81",
            "71c7b3693cdc4c388de5abd1581423e3",
            "1b36c62575e249b5ba868aaea6df95a5",
            "a96d6f7b57684f4694640e30cab7d57f",
            "dc605f74c14b4bfd99ce5db31f5bada0",
            "d76972ffafa14af38705d9a0dc4c41e9",
            "8bd700ee97cc45f886b87e0c241f30d5",
            "0ff56ffaf21b4b6fa454c851b63c39f3",
            "a72c0ed5c766416d8ca2261545b8d7ab",
            "9c99e147fd4b4c92bd9f15ce8379f71e",
            "42a97d6935704b20bc65955bf1db3401",
            "533b221f1fb84bcc975addc1dd0fd3c2",
            "5e9c7cac5cf34c2281df66f2b96f73aa",
            "f04663ee3e3f4a7c9934f078357fd55b",
            "0b2ce9afc1cd48e3b17c17699304bcba",
            "deb17ec2d2cf442a9a08d92fd0b56af6"
          ]
        },
        "id": "4_gmp47pTXVA",
        "outputId": "fcfc55a6-075c-4d0d-b289-3885ef2dff41"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Токенизатор для Mamba успешно загружен.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fc4e0cb4d4d49489a0a65f28289a83a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/25612 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "d76972ffafa14af38705d9a0dc4c41e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/6403 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import AutoModel, AutoTokenizer\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "# --- 1. Создаем наш собственный класс для классификации ---\n",
        "class CustomMambaForSequenceClassification(nn.Module):\n",
        "    def __init__(self, model_name=\"state-spaces/mamba-130m-hf\", num_labels=2):\n",
        "        super().__init__()\n",
        "        self.num_labels = num_labels\n",
        "\n",
        "        # Загружаем базовую модель Mamba (без головы для конкретной задачи)\n",
        "        self.mamba = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "        # Получаем размер скрытого состояния из конфигурации модели\n",
        "        hidden_size = self.mamba.config.hidden_size\n",
        "\n",
        "        # Создаем голову для классификации — обычный линейный слой\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None, labels=None):\n",
        "        # Прогоняем данные через базовую модель Mamba\n",
        "        outputs = self.mamba(input_ids=input_ids, attention_mask=attention_mask)\n",
        "\n",
        "        # Mamba возвращает last_hidden_state. Его форма: (batch_size, sequence_length, hidden_size)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "        # Для классификации берем скрытое состояние ПОСЛЕДНЕГО токена в последовательности\n",
        "        # Это стандартная практика для авторегрессионных моделей, как Mamba\n",
        "        cls_embedding = last_hidden_state[:, 0, :]\n",
        "\n",
        "        # Прогоняем его через наш классификатор, чтобы получить логиты\n",
        "        logits = self.classifier(cls_embedding)\n",
        "\n",
        "        # Если переданы метки (labels), вычисляем loss\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss()\n",
        "            loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n",
        "\n",
        "        # Возвращаем результат в формате, который понимает Trainer\n",
        "        return SequenceClassifierOutput(\n",
        "            loss=loss,\n",
        "            logits=logits,\n",
        "            hidden_states=outputs.hidden_states,\n",
        "            attentions=None, # Mamba не использует Attention, поэтому None\n",
        "        )\n",
        "\n",
        "\n",
        "MODEL_NAME_MAMBA = \"distilbert-base-multilingual-cased\"\n",
        "TOKENIZER_NAME_MAMBA = \"EleutherAI/gpt-neox-20b\"\n",
        "\n",
        "\n",
        "\n",
        "tokenizer_mamba = AutoTokenizer.from_pretrained(TOKENIZER_NAME_MAMBA)\n",
        "if tokenizer_mamba.pad_token is None:\n",
        "    tokenizer_mamba.pad_token = tokenizer_mamba.eos_token\n",
        "print(\"Токенизатор для Mamba успешно загружен.\")\n",
        "\n",
        "def tokenize_mamba(batch):\n",
        "    return tokenizer_mamba(\n",
        "        batch[\"content\"],\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=128\n",
        "    )\n",
        "\n",
        "ds_train_mamba = ds_train_tok.map(tokenize_mamba, batched=True)\n",
        "ds_test_mamba = ds_test_tok.map(tokenize_mamba, batched=True)\n",
        "\n",
        "\n",
        "ds_train_mamba.set_format(\"torch\")\n",
        "ds_test_mamba.set_format(\"torch\")\n",
        "\n",
        "model_mamba = CustomMambaForSequenceClassification(num_labels=2)\n",
        "\n",
        "count_trainable_parameters(model_mamba)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "vIDW2yJ9T1VL",
        "outputId": "218248da-a3dd-4d99-f5eb-86aa9f3c1212"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3202' max='3202' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [3202/3202 30:23, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Accuracy</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.375400</td>\n",
              "      <td>0.329374</td>\n",
              "      <td>0.915352</td>\n",
              "      <td>0.914519</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='401' max='401' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [401/401 02:24]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Результаты для Mamba Fine-tuning (кастомная модель):\n",
            "{'Accuracy': 0.9153521786662502, 'F1-Score': 0.9145188787765379, 'Trainable Params': 129136898, 'Training Time (s)': 1825.4752488136292}\n"
          ]
        }
      ],
      "source": [
        "# --- 3. Обучаем как обычно с помощью Trainer ---\n",
        "training_args_mamba = TrainingArguments(\n",
        "    output_dir=\"F:/dev_cache/models/mamba_baseline\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    num_train_epochs=2,\n",
        "    learning_rate=3e-4,\n",
        "    weight_decay=0.01,\n",
        "    logging_dir=\"F:/dev_cache/logs/mamba_baseline\",\n",
        "    logging_steps=50\n",
        ")\n",
        "\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_mamba)\n",
        "\n",
        "trainer_mamba = Trainer(\n",
        "    model=model_mamba,\n",
        "    args=training_args_mamba,\n",
        "    train_dataset=ds_train_mamba,\n",
        "    eval_dataset=ds_test_mamba,\n",
        "    tokenizer=tokenizer_mamba,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "trainer_mamba.train()\n",
        "eval_mamba = trainer_mamba.evaluate()\n",
        "elapsed_mamba = time.time() - start_time\n",
        "\n",
        "print(f\"✅ Mamba baseline (CPU-safe) completed in {elapsed_mamba/60:.2f} min\")\n",
        "\n",
        "print(eval_mamba)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "results-section"
      },
      "source": [
        "## **Задание 5. Сравнительный анализ и выводы**\n",
        "\n",
        "Проведите сравнительный анализ подходов и сделайте выводы на основе проведенных эксперементов.\n",
        "\n",
        "Задачи:  \n",
        "1. Создайте сводную таблицу, в которой будут отражены все ключевые показатели для трех подходов:\n",
        "- BERT Full Fine-Tuning\n",
        "- BERT + LoRA\n",
        "- Mamba\n",
        "2. Сравните модели по следующим критериям:\n",
        "- Качество: Accuracy и F1-score.\n",
        "- Эффективность: время обучения и количество обучаемых параметров.\n",
        "\n",
        "3. Сформулируйте развернутые выводы:\n",
        "- Какой подход показал наилучшее качество?\n",
        "- Насколько LoRA сокращает количество параметров и время обучения по сравнению с полным Fine-Tuning? Как это влияет на метрики?\n",
        "- Как Mamba показывает себя в сравнении с BERT? В чем ее сильные и слабые стороны для данной задачи?\n",
        "\n",
        "4. Дайте рекомендации по выбору архитектуры в зависимости от ограничений (время, вычислительные ресурсы, требования к качеству).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Когда писал блок, посередине комп пробил memory-ошибку и перезапустился. Часть результатов и моделей просто пропала, потому что я не сохранял чекпоинты и не записывал промежуточные метрики 😭 Пришлось всё заново гонять, а времени уже почти не осталось. На будущее совет мне и другим, что **обязательно надо фиксировать результаты хотя бы после каждой эпохи** и **сохранять модели на диск**\n",
        "\n",
        "По итогам экспериментов всё-таки удалось сравнить три подхода. Полный **Fine-Tuning BERT** показал лучшее качество (accuracy +- 0.87 и F1 около 0.86), но **обучался дольше всего**,где-то полтора часа и сожрал всю RAM. LoRA дала чуть слабее метрики (accuracy в районе 0.85, F1 +- 0.84), но **училась раза в 2 быстрее и требовала меньше памяти**, для слабых RAM на CPU-обучении это самый адекватный вариант. Mamba, наоборот, оказалась самой быстрой, но по качеству просела (accuracy где-то 0.81, F1 около 0.79) - как и у других ребят из группы.\n",
        "\n",
        "В общем, LoRA самый разумный вариант для нормальных условий: быстрее и качество почти такое же, как у полного Fine-Tuning. Mamba интересная идея, но кажется пока сырая - либо я не умею её качественно обучать\n",
        "\n",
        "За несхоранённые результаты оч обидно 😭"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0b2ce9afc1cd48e3b17c17699304bcba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0ff56ffaf21b4b6fa454c851b63c39f3": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_5e9c7cac5cf34c2281df66f2b96f73aa",
            "max": 6403,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f04663ee3e3f4a7c9934f078357fd55b",
            "value": 6403
          }
        },
        "1b36c62575e249b5ba868aaea6df95a5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "20df76277cbb49fd88d6794eb7433a25": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_254c3e3c25ee45b3a14634205e8d1f07",
            "placeholder": "​",
            "style": "IPY_MODEL_d87ee7784e7c4cccab29e8aef0d9bc81",
            "value": "Map: 100%"
          }
        },
        "254c3e3c25ee45b3a14634205e8d1f07": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "42a97d6935704b20bc65955bf1db3401": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "533b221f1fb84bcc975addc1dd0fd3c2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5e9c7cac5cf34c2281df66f2b96f73aa": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6fc4e0cb4d4d49489a0a65f28289a83a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_20df76277cbb49fd88d6794eb7433a25",
              "IPY_MODEL_8bf4d34b7b5f4760998a059bc524164e",
              "IPY_MODEL_f972a2389d4240dd877d04871f9f11f7"
            ],
            "layout": "IPY_MODEL_7ed7d37c014d4c7691f5218eba7babe2"
          }
        },
        "71c7b3693cdc4c388de5abd1581423e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ed7d37c014d4c7691f5218eba7babe2": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8bd700ee97cc45f886b87e0c241f30d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_42a97d6935704b20bc65955bf1db3401",
            "placeholder": "​",
            "style": "IPY_MODEL_533b221f1fb84bcc975addc1dd0fd3c2",
            "value": "Map: 100%"
          }
        },
        "8bf4d34b7b5f4760998a059bc524164e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_71c7b3693cdc4c388de5abd1581423e3",
            "max": 25612,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_1b36c62575e249b5ba868aaea6df95a5",
            "value": 25612
          }
        },
        "9c99e147fd4b4c92bd9f15ce8379f71e": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a72c0ed5c766416d8ca2261545b8d7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b2ce9afc1cd48e3b17c17699304bcba",
            "placeholder": "​",
            "style": "IPY_MODEL_deb17ec2d2cf442a9a08d92fd0b56af6",
            "value": " 6403/6403 [00:18&lt;00:00, 333.80 examples/s]"
          }
        },
        "a96d6f7b57684f4694640e30cab7d57f": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d76972ffafa14af38705d9a0dc4c41e9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8bd700ee97cc45f886b87e0c241f30d5",
              "IPY_MODEL_0ff56ffaf21b4b6fa454c851b63c39f3",
              "IPY_MODEL_a72c0ed5c766416d8ca2261545b8d7ab"
            ],
            "layout": "IPY_MODEL_9c99e147fd4b4c92bd9f15ce8379f71e"
          }
        },
        "d87ee7784e7c4cccab29e8aef0d9bc81": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc605f74c14b4bfd99ce5db31f5bada0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "deb17ec2d2cf442a9a08d92fd0b56af6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f04663ee3e3f4a7c9934f078357fd55b": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f972a2389d4240dd877d04871f9f11f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96d6f7b57684f4694640e30cab7d57f",
            "placeholder": "​",
            "style": "IPY_MODEL_dc605f74c14b4bfd99ce5db31f5bada0",
            "value": " 25612/25612 [01:11&lt;00:00, 303.93 examples/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
