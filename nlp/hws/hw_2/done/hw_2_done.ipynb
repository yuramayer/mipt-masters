{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "header"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "# Выбран кодовый формат\n",
        "```\n",
        "\n",
        "# Домашнее задание 2. Поисковая система для документов\n",
        "\n",
        "**Модуль 2. Классический поиск и рекуррентные архитектуры**\n",
        "\n",
        "**ФИО студента:** Майер Юрий Алексеевич\n",
        "\n",
        "**Дата выполнения:** 28 сентября 25\n",
        "\n",
        "\n",
        "## Описание задания\n",
        "\n",
        "В этом задании вы разработаете полнофункциональную поисковую систему, включающую:\n",
        "1. **Предобработку корпуса.**\n",
        "2. **BM25.**\n",
        "3. **Векторный поиск** — на основе эмбеддингов.\n",
        "4. **Гибридный поиск** — комбинация BM25 и векторного поиска.\n",
        "5. **Выбор метрики и оценку качества** — для конкретной задачи.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "setup"
      },
      "source": [
        "## Установка и импорт библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "imports"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import math\n",
        "import time\n",
        "from typing import List, Dict, Tuple, Optional, Set\n",
        "from collections import defaultdict, Counter\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "# NLP\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "import pymorphy3\n",
        "\n",
        "# Векторный поиск\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# BM25\n",
        "from rank_bm25 import BM25Okapi\n",
        "\n",
        "# Инициализация морфологического анализатора\n",
        "morph = pymorphy3.MorphAnalyzer()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part1"
      },
      "source": [
        "## Часть 1. Подготовка данных\n",
        "\n",
        "1. Загрузите и изучите предложенный датасет.  \n",
        "2. Реализуйте функцию предобработки текста, которая включает:\n",
        "- Лемматизацию с использованием pymorphy3.\n",
        "- Удаление стоп-слов и пунктуации.  \n",
        "3. Обработайте весь корпус документов и сохраните результат для последующих шагов.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "load_data"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Загружаем корпус документов\n",
        "ds = load_dataset(\"MLNavigator/russian-retrieval\",\n",
        "                  cache_dir='E:/hugging_face')\n",
        "df = pd.DataFrame(ds['train'])\n",
        "questions_df = df[['text','q']]\n",
        "\n",
        "\n",
        "# Уберем дубли, так как датасет имеет соответствие много вопросов -> один документ\n",
        "documents = df['text'].drop_duplicates().to_list()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to e:\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('punkt', download_dir=r\"e:\\nltk_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to e:\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords', download_dir=r\"e:\\nltk_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "nltk.data.path.insert(0, r\"e:\\nltk_data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "preprocessing"
      },
      "outputs": [],
      "source": [
        "import string\n",
        "\n",
        "\n",
        "russian_stopwords = set(stopwords.words(\"russian\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [],
      "source": [
        "def preprocess_text(text: str) -> str:\n",
        "    \"\"\"\n",
        "    Базовая предобработка текста:\n",
        "    1 Приведёём к нижнему регистру\n",
        "    2. Токенизируем\n",
        "    3. Фильтруем по пунктуации и стоп-словам\n",
        "    4 Лемматизация\n",
        "    \"\"\"\n",
        "    # В нижний регистр\n",
        "    text = text.lower()\n",
        "    \n",
        "    # Токенизация\n",
        "    tokens = word_tokenize(text, language=\"russian\")\n",
        "    \n",
        "    cleaned_tokens = []\n",
        "    for token in tokens:\n",
        "        # Пропускаем пунктуацию и стоп-слова\n",
        "        if token in russian_stopwords or token in string.punctuation:\n",
        "            continue\n",
        "        \n",
        "        # Лемматизация\n",
        "        lemma = morph.parse(token)[0].normal_form\n",
        "        cleaned_tokens.append(lemma)\n",
        "    \n",
        "    return \" \".join(cleaned_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "778c7b86b12b49e6ba684a25a9fe3086",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Preprocessing corpus:   0%|          | 0/9076 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>original</th>\n",
              "      <th>processed</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>В протерозойских отложениях органические остат...</td>\n",
              "      <td>протерозойский отложение органический остаток ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Кишечник млекопитающего подразделяется на тонк...</td>\n",
              "      <td>кишечник млекопитающее подразделяться тонкий т...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Город Байконур и космодром Байконур вместе обр...</td>\n",
              "      <td>город байконур космодром байконур вместе образ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Вскоре после прибытия Колумба из Вест-Индии во...</td>\n",
              "      <td>вскоре прибытие колумб вест-индия возникнуть н...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Около Порт-Артура ночью на 27 января 1904 года...</td>\n",
              "      <td>около порт-артур ночью 27 январь 1904 год нача...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                            original  \\\n",
              "0  В протерозойских отложениях органические остат...   \n",
              "1  Кишечник млекопитающего подразделяется на тонк...   \n",
              "2  Город Байконур и космодром Байконур вместе обр...   \n",
              "3  Вскоре после прибытия Колумба из Вест-Индии во...   \n",
              "4  Около Порт-Артура ночью на 27 января 1904 года...   \n",
              "\n",
              "                                           processed  \n",
              "0  протерозойский отложение органический остаток ...  \n",
              "1  кишечник млекопитающее подразделяться тонкий т...  \n",
              "2  город байконур космодром байконур вместе образ...  \n",
              "3  вскоре прибытие колумб вест-индия возникнуть н...  \n",
              "4  около порт-артур ночью 27 январь 1904 год нача...  "
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# обернём в tqdm\n",
        "processed_documents = []\n",
        "\n",
        "for doc in tqdm(documents, desc=\"Preprocessing corpus\"):\n",
        "    processed_documents.append(preprocess_text(doc))\n",
        "\n",
        "\n",
        "docs_df = pd.DataFrame({\n",
        "    \"original\": documents,\n",
        "    \"processed\": processed_documents\n",
        "})\n",
        "\n",
        "docs_df.head()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.mkdir('data')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [],
      "source": [
        "docs_df.to_csv(\"data/processed_documents.csv\", index=False, encoding=\"utf-8\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part2"
      },
      "source": [
        "## Часть 2. Реализация BM25\n",
        "\n",
        "1. Постройте инвертированный индекс для корпуса. Индекс должен содержать частоту термина в документе (TF) и документную частоту (DF).\n",
        "2. Реализуйте функцию поиска BM25 с нуля. Формула для ранжирования:\n",
        "score(D, Q) = Σ IDF(qi) * (f(qi, D) * (k1 + 1)) / (f(qi, D) + k1 * (1 - b + b * |D| / avgdl))\n",
        "3. Проведите оптимизацию гиперпараметра k1, чтобы улучшить качество поиска."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Постриом инвертированный индекс:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import defaultdict\n",
        "import math\n",
        "\n",
        "# Строим индекс по предобработанным документам\n",
        "inverted_index = defaultdict(dict)  # term -> {doc_id: freq}\n",
        "doc_lengths = []  # длины документов (в токенах)\n",
        "\n",
        "for doc_id, doc in enumerate(docs_df[\"processed\"]):\n",
        "    tokens = doc.split()\n",
        "    doc_lengths.append(len(tokens))\n",
        "    freqs = Counter(tokens)\n",
        "    for term, count in freqs.items():\n",
        "        inverted_index[term][doc_id] = count\n",
        "\n",
        "N = len(docs_df)    # общее число документов\n",
        "avgdl = sum(doc_lengths) / N    # средняя длина документа"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Создадим класс под BM25:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "class BM25:\n",
        "    def __init__(self, inverted_index, doc_lengths, avgdl, k1=1.5, b=0.75):\n",
        "        self.index = inverted_index\n",
        "        self.doc_lengths = doc_lengths\n",
        "        self.avgdl = avgdl\n",
        "        self.N = len(doc_lengths)\n",
        "        self.k1 = k1\n",
        "        self.b = b\n",
        "\n",
        "    def idf(self, term):\n",
        "\n",
        "        df = len(self.index.get(term, {}))\n",
        "        if df == 0:\n",
        "            return 0\n",
        "        return math.log(1 + (self.N - df + 0.5) / (df + 0.5))\n",
        "\n",
        "    def score(self, query, doc_id):\n",
        "        tokens = query.split()\n",
        "        score = 0.0\n",
        "        for term in tokens:\n",
        "            if doc_id not in self.index.get(term, {}):\n",
        "                continue\n",
        "            f = self.index[term][doc_id]  # term frequency\n",
        "            idf = self.idf(term)\n",
        "            dl = self.doc_lengths[doc_id]\n",
        "            denom = f + self.k1 * (1 - self.b + self.b * dl / self.avgdl)\n",
        "            score += idf * (f * (self.k1 + 1)) / denom\n",
        "        return score\n",
        "\n",
        "    def search(self, query, top_k=5):\n",
        "\n",
        "        scores = []\n",
        "        for doc_id in range(self.N):\n",
        "            s = self.score(query, doc_id)\n",
        "            if s > 0:\n",
        "                scores.append((doc_id, s))\n",
        "        scores.sort(key=lambda x: x[1], reverse=True)\n",
        "        return scores[:top_k]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "bm25 = BM25(inverted_index, doc_lengths, avgdl, k1=1.5, b=0.75)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "bm25_implementation"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Тестирование BM25:\n",
            "Запрос: 'машинное обучение'\n",
            "  [10.658] Особенность программного обеспечения состоит в том, что оно производится в одной...\n",
            "  [10.270] Промышленный переворот, произошедший с 60-х годов XVIII до первой четверти XIX в...\n",
            "  [8.999] Цифровые обучающие игры отличаются от традиционных обучающих игр и не основанног...\n",
            "\n",
            "Запрос: 'нейронные сети'\n",
            "  [18.523] Области мозга, постоянно используемые, когда человек занят вопросами морали, был...\n",
            "  [7.732] Двусторонние рынки (двусторонние сети) — сетевые рынки, которые имеют две группы...\n",
            "  [7.476] Во множестве сетей пользователи являются гомогенными, то есть они выполняют один...\n",
            "\n",
            "Запрос: 'поисковые системы BM25'\n",
            "  [17.158] Для поиска информации с помощью поисковой системы пользователь формулирует поиск...\n",
            "  [16.063] Полезность поисковой системы зависит от релевантности найденных ею страниц. Хоть...\n",
            "  [15.075] Когда пользователь вводит запрос в поисковую систему (обычно при помощи ключевых...\n",
            "\n",
            "Запрос: 'Python программирование'\n",
            "  [11.529] В теории языков программирования, как подразделе информатики, изучают проектиров...\n",
            "  [10.061] Регулирование (системы регулирования) включают в себя контроль (системы контроля...\n",
            "  [9.661] В некоторых университетах информатика преподаётся в качестве теоретического изуч...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Реализация поиска используя BM25\n",
        "\n",
        "# Тестовые запросы\n",
        "test_queries = [\n",
        "    \"машинное обучение\",\n",
        "    \"нейронные сети\",\n",
        "    \"поисковые системы BM25\",\n",
        "    \"Python программирование\"\n",
        "]\n",
        "\n",
        "print(\"Тестирование BM25:\")\n",
        "for query in test_queries:\n",
        "    results = bm25.search(preprocess_text(query), top_k=3)\n",
        "    print(f\"Запрос: '{query}'\")\n",
        "    for doc_id, score in results:\n",
        "        snippet = docs_df.loc[doc_id, \"original\"][:80].replace(\"\\n\", \" \")\n",
        "        print(f\"  [{score:.3f}] {snippet}...\")\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Вроде находим по смыслу!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part3"
      },
      "source": [
        "## Часть 3. Векторный поиск\n",
        "\n",
        "1. Используйте предобученную модель sentence-transformers для получения векторных представлений (эмбеддингов) всех документов.\n",
        "2. Создайте индекс для быстрого поиска ближайших соседей с помощью faiss-cpu.\n",
        "3. Реализуйте функцию векторного поиска, которая по запросу находит top-k наиболее близких документов.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vector_search"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer('BAAI/bge-m3', device='cuda')\n",
        "\n",
        "\n",
        "# Тестируем\n",
        "print(\"Тестирование векторного поиска:\")\n",
        "for query in test_queries:\n",
        "    results = vector_search.search(query, top_k=3)\n",
        "    print(f\"Запрос: '{query}'\")\n",
        "    for doc_id, score in results:\n",
        "        doc = documents[doc_id]\n",
        "        print(f\"  [{score:.3f}] {doc.title[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part4"
      },
      "source": [
        "## Часть 4. Гибридный поиск\n",
        "\n",
        "1. Разработайте функцию, которая комбинирует результаты ранжирования от BM25 и векторного поиска.\n",
        "2. Реализуйте механизм взвешивания скоров с помощью параметра α:\n",
        "hybrid_score = α * bm25_score + (1 - α) * vector_score\n",
        "3. Проведите автоматическую оптимизацию параметра α на валидационном наборе данных.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hybrid_search"
      },
      "outputs": [],
      "source": [
        "# Тестируем гибридный поиск\n",
        "print(\"Тестирование гибридного поиска:\")\n",
        "for query in test_queries:\n",
        "    results = hybrid_search.search(query, top_k=3)\n",
        "    print(f\"Запрос: '{query}'\")\n",
        "    for doc_id, score in results:\n",
        "        doc = documents[doc_id]\n",
        "        print(f\"  [{score:.3f}] {doc.title[:50]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "part6"
      },
      "source": [
        "## Часть 5. Оценка качества\n",
        "\n",
        "1. Выберите и **обоснуйте метрику** для оценки качества вашей поисковой системы (например, MRR, MAP@k или NDCG@k). **Обязательно подумайте о том, какой топ-к нужно выбрать исходя из данных**.\n",
        "2. **Создайте небольшой датасет для оценки**, состоящий из запросов и релевантных им документов.  \n",
        "3. **Сравните качество** всех трех реализованных подходов (BM25, векторный, гибридный) на вашем датасете.  \n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "metrics"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
